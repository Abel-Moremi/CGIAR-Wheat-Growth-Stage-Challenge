{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapted from Pytorch transfer learning [tutorial]( https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ExifTags\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wheat(Dataset):\n",
    "    def __init__(self, imgs, gts, split_type, transform):\n",
    "        self.imgs = imgs\n",
    "        self.gts = gts\n",
    "                   \n",
    "        self.split_type = split_type\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        if self.split_type == 'test':\n",
    "            y = 0\n",
    "        else:\n",
    "            y = self.gts[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)\n",
    "        return img, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(imgs_path, lbl_path):\n",
    "    lbl_df = pd.read_csv(lbl_path)\n",
    "    gt = lbl_df['growth_stage'].to_numpy().astype(np.float32)\n",
    "    lbl_quality = lbl_df['label_quality'].to_numpy()\n",
    "    train_imgs_fname_set = lbl_df['UID'].tolist()\n",
    "    \n",
    "    imgs_fname_arr = os.listdir(imgs_path)\n",
    "    test_id_arr = []\n",
    "    imgs_arr = np.zeros((lbl_df.shape[0], 224, 224, 3), dtype = np.uint8)\n",
    "    test_imgs_arr = np.zeros((len(imgs_fname_arr) - lbl_df.shape[0], 224, 224, 3), dtype = np.uint8)\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    for fname in tqdm(imgs_fname_arr):\n",
    "        img = np.array(Image.open(os.path.join(imgs_path, fname)).convert('RGB').resize((224,224), Image.ANTIALIAS)).astype(np.uint8)\n",
    "        img_id = fname.split('.')[0] \n",
    "        if img_id in train_imgs_fname_set:\n",
    "            imgs_arr[i] = img\n",
    "            i += 1\n",
    "        else:\n",
    "            test_imgs_arr[j] = img\n",
    "            j += 1\n",
    "            test_id_arr.append(img_id)\n",
    "            \n",
    "    return imgs_arr, test_imgs_arr, gt, lbl_quality, test_id_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 14253/14253 [02:16<00:00, 104.10it/s]\n"
     ]
    }
   ],
   "source": [
    "#change imgs_path and lbl_path with your paths\n",
    "imgs_arr, test_imgs_arr, gt, lbl_quality, test_id_arr = read_dataset('Data/Images/', 'Data/Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only high quality data\n",
    "imgs_arr = imgs_arr[lbl_quality == 2]\n",
    "gt = gt[lbl_quality == 2]\n",
    "lbl_quality = lbl_quality[lbl_quality == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, device, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1000000.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            print('{} RMSE Loss: {:.4f}'.format(\n",
    "                phase, np.sqrt(epoch_loss)))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val RMSE Loss: {:4f}'.format(np.sqrt(best_loss)))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into 80% training and 20% validation\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1234)\n",
    "\n",
    "for train_index, val_index in sss.split(imgs_arr, gt):\n",
    "    break\n",
    "\n",
    "train_imgs_arr = imgs_arr[train_index]\n",
    "train_gt = gt[train_index]\n",
    "val_imgs_arr = imgs_arr[val_index]\n",
    "val_gt = gt[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {'train': Wheat(train_imgs_arr, train_gt, 'train', data_transforms['train']),\n",
    "                  'val': Wheat(val_imgs_arr, val_gt, 'val', data_transforms['val'])}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, device,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    res_arr = []\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)   \n",
    "            res_arr.append(outputs.detach().cpu().numpy())\n",
    "    res_arr = np.concatenate(res_arr, axis = 0)\n",
    "    return res_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets['test'] = Wheat(test_imgs_arr, None, 'test', data_transforms['val'])\n",
    "test_loader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4,shuffle=False, num_workers=16)\n",
    "test_pred = test(model_ft, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('SampleSubmission.csv')\n",
    "sub['UID'] = test_id_arr\n",
    "sub['growth_stage'] = test_pred.flatten().tolist()\n",
    "sub.to_csv('high_quality_data_resnet18_sub.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
